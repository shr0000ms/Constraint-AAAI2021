{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import itertools\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import optimizers, Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6420,)\n"
     ]
    }
   ],
   "source": [
    "names = ['Tweet', 'Label']\n",
    "df = pd.read_csv('train.csv', sep=',', names=names, header=0)\n",
    "#df_val = pd.read_csv('val.csv', sep=',', names=names, header=0)\n",
    "#df=pd.concat((df_train, df_val))\n",
    "df.dropna(how='any', inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df[\"Tweet\"] = df['Tweet'].values.astype('U')\n",
    "\n",
    "X = df['Tweet'].to_numpy()\n",
    "y = df['Label'].to_numpy()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape is (6420, 10000)\n"
     ]
    }
   ],
   "source": [
    "MAX_FEATURES = 10000\n",
    "\n",
    "cv = CountVectorizer(max_features = MAX_FEATURES)\n",
    "cv.fit(X)\n",
    "X_train = cv.transform(X)\n",
    "X_train = X_train.todense()\n",
    "X=X_train\n",
    "print('X shape is', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array(X)\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_Iso = IsolationForest(random_state=np.random.RandomState(42),n_jobs = -1)\n",
    "clf_Iso.fit(X)\n",
    "y_Iso_Forest = clf_Iso.predict(X)\n",
    "result = np.where(y_Iso_Forest == -1)\n",
    "result = list(itertools.chain.from_iterable(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_removed = np.delete(X,result,axis = 0)\n",
    "if y is None:\n",
    "    X=X_removed\n",
    "else:\n",
    "    y_removed = np.delete(y,result,axis = 0)\n",
    "X=X_removed\n",
    "y=y_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back=1\n",
    "num_samples=X.shape[0]\n",
    "num_features=X.shape[1]\n",
    "X = np.reshape(np.array(X), (num_samples, look_back, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6420, 1, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(look_back=None, input_nodes=None, activation='relu', \n",
    "                optimizer='adam', hidden_layers=2, neurons=400, hidden_units=600):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.LSTM(hidden_units, dropout=0.2, \n",
    "                                input_shape=(look_back, input_nodes)))\n",
    "    \n",
    "    for _ in range(hidden_layers):\n",
    "        model.add(keras.layers.Dense(neurons, activation=activation))\n",
    "\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, \n",
    "                    metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4280 samples, validate on 2140 samples\n",
      "Epoch 1/5\n",
      "4280/4280 [==============================] - 12s 3ms/sample - loss: 0.3948 - accuracy: 0.8089 - val_loss: 0.2159 - val_accuracy: 0.9187\n",
      "Epoch 2/5\n",
      "4280/4280 [==============================] - 9s 2ms/sample - loss: 0.1220 - accuracy: 0.9540 - val_loss: 0.2127 - val_accuracy: 0.9168\n",
      "Epoch 3/5\n",
      "4280/4280 [==============================] - 9s 2ms/sample - loss: 0.0724 - accuracy: 0.9710 - val_loss: 0.2670 - val_accuracy: 0.9093\n",
      "Epoch 4/5\n",
      "4280/4280 [==============================] - 9s 2ms/sample - loss: 0.0490 - accuracy: 0.9822 - val_loss: 0.2875 - val_accuracy: 0.9154\n",
      "Epoch 5/5\n",
      "4280/4280 [==============================] - 9s 2ms/sample - loss: 0.0415 - accuracy: 0.9839 - val_loss: 0.3244 - val_accuracy: 0.9126\n",
      "----Start Evaluating----\n",
      "2140/2140 [==============================] - 2s 931us/sample - loss: 0.3244 - accuracy: 0.9126 - loss: 0.3372 \n",
      "Testing Accuracy: 0.91261685\n",
      "Train on 4280 samples, validate on 2140 samples\n",
      "Epoch 1/5\n",
      "4280/4280 [==============================] - 12s 3ms/sample - loss: 0.3744 - accuracy: 0.8264 - val_loss: 0.2133 - val_accuracy: 0.9136\n",
      "Epoch 2/5\n",
      "4280/4280 [==============================] - 9s 2ms/sample - loss: 0.1273 - accuracy: 0.9507 - val_loss: 0.2056 - val_accuracy: 0.9243\n",
      "Epoch 3/5\n",
      "4280/4280 [==============================] - 9s 2ms/sample - loss: 0.0618 - accuracy: 0.9766 - val_loss: 0.2749 - val_accuracy: 0.9234\n",
      "Epoch 4/5\n",
      "4280/4280 [==============================] - 9s 2ms/sample - loss: 0.0590 - accuracy: 0.9776 - val_loss: 0.2753 - val_accuracy: 0.9252\n",
      "Epoch 5/5\n",
      "4280/4280 [==============================] - 9s 2ms/sample - loss: 0.0404 - accuracy: 0.9846 - val_loss: 0.3121 - val_accuracy: 0.9206\n",
      "----Start Evaluating----\n",
      "2140/2140 [==============================] - 2s 1ms/sample - loss: 0.3121 - accuracy: 0.9206\n",
      "Testing Accuracy: 0.9205608\n",
      "Train on 4280 samples, validate on 2140 samples\n",
      "Epoch 1/5\n",
      "4280/4280 [==============================] - 12s 3ms/sample - loss: 0.4118 - accuracy: 0.7914 - val_loss: 0.2061 - val_accuracy: 0.9150\n",
      "Epoch 2/5\n",
      "4280/4280 [==============================] - 9s 2ms/sample - loss: 0.1349 - accuracy: 0.9484 - val_loss: 0.2008 - val_accuracy: 0.9271\n",
      "Epoch 3/5\n",
      "4280/4280 [==============================] - 9s 2ms/sample - loss: 0.0721 - accuracy: 0.9724 - val_loss: 0.2250 - val_accuracy: 0.9229\n",
      "Epoch 4/5\n",
      "4280/4280 [==============================] - 9s 2ms/sample - loss: 0.0528 - accuracy: 0.9790 - val_loss: 0.2510 - val_accuracy: 0.9201\n",
      "Epoch 5/5\n",
      "4280/4280 [==============================] - 9s 2ms/sample - loss: 0.0418 - accuracy: 0.9841 - val_loss: 0.2587 - val_accuracy: 0.9173\n",
      "----Start Evaluating----\n",
      "2140/2140 [==============================] - 2s 949us/sample - loss: 0.2587 - accuracy: 0.9173 - loss: 0.2642 - accura\n",
      "Testing Accuracy: 0.91728973\n",
      "Mean testing accuracy: 0.9168224533398946\n"
     ]
    }
   ],
   "source": [
    "epochs = 5 # can change this\n",
    "kf = KFold(n_splits=3, random_state=None)\n",
    "acc_list = []\n",
    "X_train = None # init\n",
    "X_test = None # init\n",
    "y_test = None #init\n",
    "# Doing cross validation testing\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    model = create_model(look_back=look_back, input_nodes=num_features)\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size)\n",
    "    print(\"----Start Evaluating----\")\n",
    "    _, acc = model.evaluate(X_test, y_test, verbose=1)\n",
    "    acc_list.append(acc)\n",
    "    print(\"Testing Accuracy:\", acc)\n",
    "print(\"Mean testing accuracy:\", sum(acc_list) / len(acc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val=pd.read_csv('val.csv', names=names, sep=',', header=0)\n",
    "df_val.dropna(how='any', inplace=True)\n",
    "df_val.reset_index(drop=True, inplace=True)\n",
    "df_val[\"Tweet\"] = df_val['Tweet'].values.astype('U')\n",
    "X_val = df_val['Tweet'].to_numpy()\n",
    "y_val = df_val['Label'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2140, 10000)\n"
     ]
    }
   ],
   "source": [
    "X_val=cv.transform(X_val)\n",
    "X_val=X_val.todense()\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2140, 1, 10000)\n",
      "2140/2140 [==============================] - 2s 955us/sample - loss: 0.2699 - accuracy: 0.9150\n"
     ]
    }
   ],
   "source": [
    "num_samples_val=X_val.shape[0]\n",
    "num_features_val=X_val.shape[1]\n",
    "X_val = np.reshape(np.array(X_val), (num_samples_val, look_back, num_features_val))\n",
    "print(X_val.shape)\n",
    "_, acc_val = model.evaluate(X_val, y_val, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_probs = model.predict(X_val).reshape(X_val.shape[0])\n",
    "\n",
    "np.savetxt('lstm_probs.csv', lstm_probs, delimiter=',', header='probs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_preds = (lstm_probs >= 0.5).astype(\"int32\")\n",
    "np.savetxt('lstm_preds.csv', lstm_preds, delimiter=',', header='preds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1029   91]\n",
      " [  91  929]]\n"
     ]
    }
   ],
   "source": [
    "lstm_cm = confusion_matrix(np.array(y_val), lstm_preds)\n",
    "print(lstm_cm) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6420,)\n"
     ]
    }
   ],
   "source": [
    "names = ['Tweet', 'Label']\n",
    "df = pd.read_csv('train.csv', sep=',', names=names, header=0)\n",
    "#df_val = pd.read_csv('val.csv', sep=',', names=names, header=0)\n",
    "#df=pd.concat((df_train, df_val))\n",
    "df.dropna(how='any', inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df[\"Tweet\"] = df['Tweet'].values.astype('U')\n",
    "X = df['Tweet'].to_numpy()\n",
    "y = df['Label'].to_numpy()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape is (6420, 10000)\n"
     ]
    }
   ],
   "source": [
    "MAX_FEATURES = 10000\n",
    "tfidf = TfidfVectorizer(max_features = MAX_FEATURES)\n",
    "tfidf.fit(X)\n",
    "X_train = tfidf.transform(X)\n",
    "X_train = X_train.todense()\n",
    "X=X_train\n",
    "print('X shape is', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array(X)\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_Iso = IsolationForest(random_state=np.random.RandomState(42),n_jobs = -1)\n",
    "clf_Iso.fit(X)\n",
    "y_Iso_Forest = clf_Iso.predict(X)\n",
    "result = np.where(y_Iso_Forest == -1)\n",
    "result = list(itertools.chain.from_iterable(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_removed = np.delete(X,result,axis = 0)\n",
    "if y is None:\n",
    "    X=X_removed\n",
    "else:\n",
    "    y_removed = np.delete(y,result,axis = 0)\n",
    "X=X_removed\n",
    "y=y_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Start Evaluating----\n",
      "Testing Accuracy: 0.9074766355140187\n",
      "----Start Evaluating----\n",
      "Testing Accuracy: 0.9186915887850468\n",
      "----Start Evaluating----\n",
      "Testing Accuracy: 0.9121495327102803\n",
      "Mean testing accuracy: 0.9127725856697819\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=3)\n",
    "svm = SVC(C=0.25, kernel='linear', probability=True)\n",
    "acc_list = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    svm.fit(X_train, y_train)\n",
    "    print(\"----Start Evaluating----\")\n",
    "    acc = svm.score(X_test, y_test)\n",
    "    acc_list.append(acc)\n",
    "    print(\"Testing Accuracy:\", acc)\n",
    "print(\"Mean testing accuracy:\", sum(acc_list) / len(acc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2140, 10000)\n",
      "Validation accuracy: 0.9065420560747663\n"
     ]
    }
   ],
   "source": [
    "df_val=pd.read_csv('val.csv', names=names, sep=',', header=0)\n",
    "df_val.dropna(how='any', inplace=True)\n",
    "df_val.reset_index(drop=True, inplace=True)\n",
    "df_val[\"Tweet\"] = df_val['Tweet'].values.astype('U')\n",
    "X_val = df_val['Tweet'].to_numpy()\n",
    "y_val = df_val['Label'].to_numpy()\n",
    "X_val=tfidf.transform(X_val)\n",
    "X_val=X_val.todense()\n",
    "print(X_val.shape)\n",
    "acc_val = svm.score(X_val, y_val)\n",
    "print('Validation accuracy:', acc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_probs = svm.predict_proba(X_val)[:, 1]\n",
    "np.savetxt('svm_probs.csv', svm_probs, delimiter=',', header='probs')\n",
    "svm_preds = svm.predict(X_val)\n",
    "np.savetxt('svm_preds.csv', svm_preds, delimiter=',', header='preds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[988 132]\n",
      " [ 68 952]]\n"
     ]
    }
   ],
   "source": [
    "svm_cm = confusion_matrix(np.array(y_val), svm_preds)\n",
    "print(svm_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOGREG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6420,)\n"
     ]
    }
   ],
   "source": [
    "names = ['Tweet', 'Label']\n",
    "df = pd.read_csv('train.csv', sep=',', names=names, header=0)\n",
    "#df_val = pd.read_csv('val.csv', sep=',', names=names, header=0)\n",
    "#df=pd.concat((df_train, df_val))\n",
    "df.dropna(how='any', inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df[\"Tweet\"] = df['Tweet'].values.astype('U')\n",
    "X = df['Tweet'].to_numpy()\n",
    "y = df['Label'].to_numpy()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape is (6420, 10000)\n"
     ]
    }
   ],
   "source": [
    "MAX_FEATURES = 10000\n",
    "tfidf = TfidfVectorizer(max_features = MAX_FEATURES)\n",
    "tfidf.fit(X)\n",
    "X_train = tfidf.transform(X)\n",
    "X_train = X_train.todense()\n",
    "X=X_train\n",
    "print('X shape is', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array(X)\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_Iso = IsolationForest(random_state=np.random.RandomState(42),n_jobs = -1)\n",
    "clf_Iso.fit(X)\n",
    "y_Iso_Forest = clf_Iso.predict(X)\n",
    "result = np.where(y_Iso_Forest == -1)\n",
    "result = list(itertools.chain.from_iterable(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_removed = np.delete(X,result,axis = 0)\n",
    "if y is None:\n",
    "    X=X_removed\n",
    "else:\n",
    "    y_removed = np.delete(y,result,axis = 0)\n",
    "X=X_removed\n",
    "y=y_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = X.shape[0]\n",
    "num_features = X.shape[1]\n",
    "X = np.reshape(np.array(X), (num_samples, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Start Evaluating----\n",
      "Testing Accuracy: 0.9247663551401869\n",
      "----Start Evaluating----\n",
      "Testing Accuracy: 0.9233644859813084\n",
      "----Start Evaluating----\n",
      "Testing Accuracy: 0.9261682242990654\n",
      "Mean testing accuracy: 0.9247663551401869\n"
     ]
    }
   ],
   "source": [
    "C = 7.74 # to be set to the best hyperpara\n",
    "solver = 'sag' # to be set to the best hyperpara\n",
    "kf = KFold(n_splits=3)\n",
    "logistic = LogisticRegression(max_iter=500, C=C, solver=solver)\n",
    "acc_list = []\n",
    "# Doing cross validation testing\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    logistic.fit(X_train, y_train)\n",
    "    print(\"----Start Evaluating----\")\n",
    "    acc = logistic.score(X_test, y_test)\n",
    "    acc_list.append(acc)\n",
    "    print(\"Testing Accuracy:\", acc)\n",
    "print(\"Mean testing accuracy:\", sum(acc_list) / len(acc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2140, 10000)\n",
      "Validation accuracy: 0.9177570093457944\n"
     ]
    }
   ],
   "source": [
    "df_val=pd.read_csv('val.csv', names=names, sep=',', header=0)\n",
    "df_val.dropna(how='any', inplace=True)\n",
    "df_val.reset_index(drop=True, inplace=True)\n",
    "df_val[\"Tweet\"] = df_val['Tweet'].values.astype('U')\n",
    "X_val = df_val['Tweet'].to_numpy()\n",
    "y_val = df_val['Label'].to_numpy()\n",
    "X_val=tfidf.transform(X_val)\n",
    "X_val=X_val.todense()\n",
    "print(X_val.shape)\n",
    "acc_val = logistic.score(X_val, y_val)\n",
    "print('Validation accuracy:', acc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_probs = logistic.predict_proba(X_val)[:, 1]\n",
    "np.savetxt('logreg_probs.csv', logreg_probs, delimiter=',', header='probs')\n",
    "logreg_preds = logistic.predict(X_val)\n",
    "np.savetxt('logreg_preds.csv', logreg_preds, delimiter=',', header='preds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1019  101]\n",
      " [  75  945]]\n"
     ]
    }
   ],
   "source": [
    "lr_cm = confusion_matrix(np.array(y_val), logreg_preds)\n",
    "print(lr_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHOQUET INTEGRAL ENSEMBLE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading probability files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.9999428e-01 9.2500099e-04 9.9949324e-01 ... 9.9434763e-01 7.6771930e-02\n",
      " 7.1197097e-09] [9.9526185e-01 1.1727647e-01 9.9278378e-01 ... 9.3014431e-01 8.9106023e-01\n",
      " 6.6690394e-05] [9.9234599e-01 1.5449223e-01 9.8246729e-01 ... 8.8683003e-01 6.7221695e-01\n",
      " 4.1882295e-04]\n"
     ]
    }
   ],
   "source": [
    "lstm_probs = pd.read_csv('lstm_probs.csv', header=0).to_numpy().astype('float32').squeeze()\n",
    "svm_probs = pd.read_csv('svm_probs.csv', header=0).to_numpy().astype('float32').squeeze()\n",
    "lr_probs = pd.read_csv('logreg_probs.csv', header=0).to_numpy().astype('float32').squeeze()\n",
    "print(lstm_probs, svm_probs, lr_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating confidence correction vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_conf = np.absolute(2*lstm_probs-1)\n",
    "svm_conf = np.absolute(2*svm_probs-1)\n",
    "lr_conf = np.absolute(2*lr_probs-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating initial fuzzy density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8441015625\n",
      "0.8253382034632034\n",
      "0.8474479302689998\n"
     ]
    }
   ],
   "source": [
    "#class 0\n",
    "ifd_lstm_0 = lstm_cm[0][0]*lstm_cm[0][0]/(np.sum(lstm_cm, axis=0)[0]*np.sum(lstm_cm, axis=1)[0])\n",
    "print(ifd_lstm_0)\n",
    "ifd_svm_0 = svm_cm[0][0]*svm_cm[0][0]/(np.sum(svm_cm, axis=0)[0]*np.sum(svm_cm, axis=1)[0])\n",
    "print(ifd_svm_0)\n",
    "ifd_lr_0 = lr_cm[0][0]*lr_cm[0][0]/(np.sum(lr_cm, axis=0)[0]*np.sum(lr_cm, axis=1)[0])\n",
    "print(ifd_lr_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8295280661284121\n",
      "0.819680196801968\n",
      "0.8370121471150602\n"
     ]
    }
   ],
   "source": [
    "#class 1\n",
    "ifd_lstm_1 = lstm_cm[1][1]*lstm_cm[1][1]/(np.sum(lstm_cm, axis=0)[1]*np.sum(lstm_cm, axis=1)[1])\n",
    "print(ifd_lstm_1)\n",
    "ifd_svm_1 = svm_cm[1][1]*svm_cm[1][1]/(np.sum(svm_cm, axis=0)[1]*np.sum(svm_cm, axis=1)[1])\n",
    "print(ifd_svm_1)\n",
    "ifd_lr_1 = lr_cm[1][1]*lr_cm[1][1]/(np.sum(lr_cm, axis=0)[1]*np.sum(lr_cm, axis=1)[1])\n",
    "print(ifd_lr_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing ifd and conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "ifd = np.array([[ifd_lstm_0, ifd_svm_0, ifd_lr_0], [ifd_lstm_1, ifd_svm_1, ifd_lr_1]])\n",
    "print(ifd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99998856, 0.9905237 , 0.984692  ],\n",
       "       [0.99815   , 0.7654471 , 0.69101554],\n",
       "       [0.9989865 , 0.98556757, 0.9649346 ],\n",
       "       ...,\n",
       "       [0.98869526, 0.8602886 , 0.77366006],\n",
       "       [0.84645617, 0.78212047, 0.3444339 ],\n",
       "       [1.        , 0.9998666 , 0.9991624 ]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = np.array([lstm_conf, svm_conf, lr_conf])\n",
    "conf.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating adjusted fuzzy density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.82951857, 0.81191266, 0.82419915],\n",
       "       [0.82799343, 0.62742181, 0.5783884 ],\n",
       "       [0.82868733, 0.80785022, 0.80766197],\n",
       "       ...,\n",
       "       [0.82015047, 0.70516155, 0.64756287],\n",
       "       [0.70215915, 0.64108866, 0.28829536],\n",
       "       [0.82952807, 0.81957086, 0.83631105]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for class 0\n",
    "afd_0 = ifd[0] * conf.T\n",
    "afd_0\n",
    "#for class 1\n",
    "afd_1 = ifd[1] * conf.T\n",
    "afd_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coefficients of quadratic for calculating lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.57583754, 2.07663095, 1.49608413],\n",
       "       [0.3117012 , 1.39562225, 1.05989238],\n",
       "       [0.56089758, 2.04063265, 1.47440444],\n",
       "       ...,\n",
       "       [0.38850489, 1.60524994, 1.2002249 ],\n",
       "       [0.13462437, 0.85818916, 0.65189868],\n",
       "       [0.58981771, 2.11006135, 1.51606776]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_0 = np.zeros(shape=(2140, 3))\n",
    "coef_0[:,0] = afd_0[:,0] * afd_0[:,1] * afd_0[:,2]\n",
    "coef_0[:,1] = afd_0[:,0] * afd_0[:, 1] + afd_0[:,1] * afd_0[:,2] + afd_0[:,0] * afd_0[:,2]\n",
    "coef_0[:,2] = afd_0[:,0] + afd_0[:,1] + afd_0[:,2] - 1\n",
    "coef_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.55509535, 2.02636285, 1.46563038],\n",
       "       [0.30047343, 1.36129644, 1.03380365],\n",
       "       [0.54069354, 1.99122438, 1.44419952],\n",
       "       ...,\n",
       "       [0.37451059, 1.566074  , 1.17287489],\n",
       "       [0.12977508, 0.83739838, 0.63154317],\n",
       "       [0.56857194, 2.05901667, 1.48540997]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_1 = np.zeros(shape=(2140, 3))\n",
    "coef_1[:,0] = afd_1[:,0] * afd_1[:,1] * afd_1[:,2]\n",
    "coef_1[:,1] = afd_1[:,0] * afd_1[:, 1] + afd_1[:,1] * afd_1[:,2] + afd_1[:,0] * afd_1[:,2]\n",
    "coef_1[:,2] = afd_1[:,0] + afd_1[:,1] + afd_1[:,2] - 1\n",
    "coef_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.99492468 -0.99386986]\n",
      " [-0.96926492 -0.96495004]\n",
      " [-0.9942189  -0.9930698 ]\n",
      " ...\n",
      " [-0.98023712 -0.97736181]\n",
      " [-0.88152216 -0.87201747]\n",
      " [-0.99552456 -0.99455682]]\n"
     ]
    }
   ],
   "source": [
    "lam = np.zeros(shape=(2140, 2))\n",
    "for i in range(lam.shape[0]):\n",
    "    lam[i, 0] = np.max(np.roots(coef_0[i, :]))\n",
    "    lam[i, 1] = np.max(np.roots(coef_1[i, :]))\n",
    "print(lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the fuzzy densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.97325691, 0.83447518],\n",
       "       [1.        , 0.85876878, 0.58559969],\n",
       "       [1.        , 0.96983898, 0.81773182],\n",
       "       ...,\n",
       "       [1.        , 0.90934467, 0.65563662],\n",
       "       [1.        , 0.77130824, 0.2918898 ],\n",
       "       [1.        , 0.97634135, 0.84673809]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd_0 = np.ones(shape=(2140, 3))\n",
    "fd_0[:, 1] = ((1 + afd_0[:,1]*lam[:,0])*(1+lam[:,0]*afd_0[:, 2]) - 1)/lam[:,0]\n",
    "fd_0[:, 2] = afd_0[:, 2]\n",
    "fd_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.97103624, 0.82419915],\n",
       "       [1.        , 0.85563612, 0.5783884 ],\n",
       "       [1.        , 0.96756404, 0.80766197],\n",
       "       ...,\n",
       "       [1.        , 0.9064254 , 0.64756287],\n",
       "       [1.        , 0.76821523, 0.28829536],\n",
       "       [1.        , 0.97419658, 0.83631105]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd_1 = np.ones(shape=(2140, 3))\n",
    "fd_1[:, 1] = ((1 + afd_1[:,1]*lam[:,1])*(1+lam[:,1]*afd_1[:, 2]) - 1)/lam[:,1]\n",
    "fd_1[:, 2] = afd_1[:, 2]\n",
    "fd_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_probs = lstm_probs.reshape((2140, 1))\n",
    "svm_probs = svm_probs.reshape((2140, 1))\n",
    "lr_probs = lr_probs.reshape((2140, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Choquet Integral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.04480437e-03, 9.92995668e-01],\n",
       "       [8.77362429e-01, 1.22004683e-01],\n",
       "       [1.54499742e-02, 9.84669175e-01],\n",
       "       ...,\n",
       "       [9.24337433e-02, 9.08103392e-01],\n",
       "       [3.59038914e-01, 6.39229105e-01],\n",
       "       [9.99636706e-01, 3.59462084e-04]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ci = np.zeros(shape=(2140, 2))\n",
    "ci[:,0] = (1-lstm_probs[:,0])*(1-fd_0[:,1]) + (1-svm_probs[:,0])*(fd_0[:,1]-fd_0[:,2]) + (1-lr_probs[:,0])*(fd_0[:,2])\n",
    "ci[:,1] = (lstm_probs[:,0])*(1-fd_1[:,1]) + (svm_probs[:,0])*(fd_1[:,1]-fd_1[:,2]) + (lr_probs[:,0])*(fd_1[:,2])\n",
    "ci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and comparing predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(ci, axis=1).astype('int')\n",
    "np.savetxt('choquet_preds.csv', preds, delimiter=',', header='Choquet Preds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val=pd.read_csv('val.csv', names=['Tweet', 'Label'], sep=',', header=0)\n",
    "true_preds = df_val['Label'].to_numpy().astype('int32')\n",
    "lstm_preds=pd.read_csv('lstm_preds.csv', header=0).to_numpy().astype('int32')\n",
    "svm_preds=pd.read_csv('svm_preds.csv', header=0).to_numpy().astype('int32')\n",
    "lr_preds=pd.read_csv('logreg_preds.csv', header=0).to_numpy().astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.9210280373831776\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.92      1120\n",
      "           1       0.91      0.92      0.92      1020\n",
      "\n",
      "    accuracy                           0.92      2140\n",
      "   macro avg       0.92      0.92      0.92      2140\n",
      "weighted avg       0.92      0.92      0.92      2140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY:',accuracy_score(true_preds, preds), end='\\n\\n')\n",
    "print(classification_report(true_preds, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.9149532710280374\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92      1120\n",
      "           1       0.91      0.91      0.91      1020\n",
      "\n",
      "    accuracy                           0.91      2140\n",
      "   macro avg       0.91      0.91      0.91      2140\n",
      "weighted avg       0.91      0.91      0.91      2140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY:',accuracy_score(true_preds, lstm_preds), end='\\n\\n')\n",
    "print(classification_report(true_preds, lstm_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.9065420560747663\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91      1120\n",
      "           1       0.88      0.93      0.90      1020\n",
      "\n",
      "    accuracy                           0.91      2140\n",
      "   macro avg       0.91      0.91      0.91      2140\n",
      "weighted avg       0.91      0.91      0.91      2140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY:',accuracy_score(true_preds, svm_preds), end='\\n\\n')\n",
    "print(classification_report(true_preds, svm_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.9177570093457944\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92      1120\n",
      "           1       0.90      0.93      0.91      1020\n",
      "\n",
      "    accuracy                           0.92      2140\n",
      "   macro avg       0.92      0.92      0.92      2140\n",
      "weighted avg       0.92      0.92      0.92      2140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY:',accuracy_score(true_preds, lr_preds), end='\\n\\n')\n",
    "print(classification_report(true_preds, lr_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
